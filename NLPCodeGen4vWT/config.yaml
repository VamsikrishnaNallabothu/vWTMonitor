llm:
  provider: openai  # or 'gemini'
  openai_api_key: "YOUR_OPENAI_API_KEY"
  gemini_api_key: "YOUR_GEMINI_API_KEY"
  model: gpt-4-turbo  # or 'gemini-pro', 'codellama', etc.
  base_url: "https://your-llm-gateway.example.com/v1"  # Set this to your OpenAI-compatible gateway or proxy

codebases:
  ZIA: "../ZIA_codebase"
  ZPA: "../ZPA_codebase"
  ZTGW: "../ZTGW_codebase"
  SMCA: "../SMCA_codebase"
  SHARED: "../shared_code"

workflow:
  execution_timeout: 120
  temp_dir: "./tmp"

logging:
  level: INFO 